{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAE Quality of Life Improvements\n",
    "\n",
    "Add config to wandb run. \n",
    "Start saving the SAE weights mid-run. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparse_autoencoder import TensorActivationStore, SparseAutoencoder, pipeline\n",
    "from sparse_autoencoder.source_data.pile_uncopyrighted import PileUncopyrightedDataset\n",
    "from sparse_autoencoder.train.sweep_config import SweepParametersRuntime\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import get_device, test_prompt\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "import torch\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_model = HookedTransformer.from_pretrained(\n",
    "    \"tiny-stories-instruct-1M\", dtype=\"float32\"\n",
    ")\n",
    "\n",
    "# test the model\n",
    "example_prompt = \"\"\"\n",
    "Once upon a time, there lived a black cat. The cat belonged to a little girl called Katie. Every day, Katie\n",
    "would take her cat for a walk in the park.\n",
    "One day, as Katie and her cat were walking around, they saw a mean looking man. He said he wanted to\n",
    "take the cat, to which she replied ”This cat belongs to\n",
    "\"\"\"\n",
    "example_answer = \" me\"\n",
    "\n",
    "\n",
    "test_prompt(example_prompt, example_answer, src_model, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To train on Tiny Stories, we're going to need the tiny stories dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, final\n",
    "from sparse_autoencoder.source_data.abstract_dataset import (\n",
    "    SourceDataset,\n",
    "    TokenizedPrompts,\n",
    ")\n",
    "\n",
    "\n",
    "class TinyStoriesSourceDataBatch(TypedDict):\n",
    "    \"\"\"Pile Uncopyrighted Source Data.\n",
    "\n",
    "    https://huggingface.co/datasets/roneneldan/TinyStories\n",
    "    \"\"\"\n",
    "\n",
    "    text: list[str]\n",
    "    meta: list[dict[str, dict[str, str]]]\n",
    "\n",
    "\n",
    "@final\n",
    "class TinyStoriesDataset(SourceDataset[TinyStoriesSourceDataBatch]):\n",
    "    \"\"\"Tiny Stories Dataset.\n",
    "\n",
    "    https://huggingface.co/datasets/roneneldan/TinyStories\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "\n",
    "    def preprocess(\n",
    "        self,\n",
    "        source_batch: TinyStoriesSourceDataBatch,\n",
    "        *,\n",
    "        context_size: int,\n",
    "    ) -> TokenizedPrompts:\n",
    "        \"\"\"Preprocess a batch of prompts.\n",
    "\n",
    "        For each prompt's `text`, tokenize it and chunk into a list of tokenized prompts of length\n",
    "        `context_size`. For the last item in the chunk, throw it away if the length is less than\n",
    "        `context_size` (i.e. if it would otherwise require padding). Then finally flatten all\n",
    "        batches to a single list of tokenized prompts.\n",
    "\n",
    "        Args:\n",
    "            source_batch: A batch of source data. For example, with The Pile dataset this would be a\n",
    "                dict including the key \"text\" with a value of a list of strings (not yet tokenized).\n",
    "            context_size: The context size to use when returning a list of tokenized prompts.\n",
    "        \"\"\"\n",
    "        prompts: list[str] = source_batch[\"text\"]\n",
    "\n",
    "        tokenized_prompts = self.tokenizer(prompts)\n",
    "\n",
    "        # Chunk each tokenized prompt into blocks of context_size, discarding the last block if too\n",
    "        # small.\n",
    "        context_size_prompts = []\n",
    "        for encoding in list(tokenized_prompts[\"input_ids\"]):  # type: ignore\n",
    "            chunks = [\n",
    "                encoding[i : i + context_size]\n",
    "                for i in range(0, len(encoding), context_size)\n",
    "                if len(encoding[i : i + context_size]) == context_size\n",
    "            ]\n",
    "            context_size_prompts.extend(chunks)\n",
    "\n",
    "        return {\"input_ids\": context_size_prompts}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: PreTrainedTokenizerBase,\n",
    "        context_size: int = 250,\n",
    "        buffer_size: int = 1000,\n",
    "        preprocess_batch_size: int = 1000,\n",
    "        dataset_path: str = \"roneneldan/TinyStories\",\n",
    "        dataset_split: str = \"train\",\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        super().__init__(\n",
    "            dataset_path=dataset_path,\n",
    "            dataset_split=dataset_split,\n",
    "            context_size=context_size,\n",
    "            buffer_size=buffer_size,\n",
    "            preprocess_batch_size=preprocess_batch_size,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an AutoEncoder for Tiny Stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "\n",
    "src_model = HookedTransformer.from_pretrained(\n",
    "    \"tiny-stories-instruct-1M\", dtype=\"float32\"\n",
    ")\n",
    "src_d_model: int = src_model.cfg.d_model  # type: ignore\n",
    "\n",
    "tokenizer: PreTrainedTokenizerBase = src_model.tokenizer  # type: ignore\n",
    "max_items = 2_000_000\n",
    "store = TensorActivationStore(max_items, src_d_model, device)\n",
    "\n",
    "# Make Autoencoder\n",
    "src_model_activation_hook_point = \"blocks.0.hook_resid_pre\"\n",
    "autoencoder = SparseAutoencoder(src_d_model, src_d_model * 8, torch.zeros(src_d_model))\n",
    "autoencoder.to(device)\n",
    "\n",
    "# Make Source Data\n",
    "tokenizer: PreTrainedTokenizerBase = src_model.tokenizer  # type: ignore\n",
    "source_data = TinyStoriesDataset(tokenizer=tokenizer)\n",
    "\n",
    "# hyper parameter\n",
    "num_iterations = 3\n",
    "\n",
    "\n",
    "sweep_config = SweepParametersRuntime(\n",
    "    lr=1e-3,\n",
    "    batch_size=2048,\n",
    "    l1_coefficient=1e-3,\n",
    ")\n",
    "\n",
    "wandb.init(\n",
    "    project=\"sparse-autoencoder\", dir=\".cache/wandb\", config=sweep_config.__dict__\n",
    ")\n",
    "\n",
    "pipeline(\n",
    "    src_model=src_model,\n",
    "    src_model_activation_hook_point=src_model_activation_hook_point,\n",
    "    src_model_activation_layer=0,  # why do we need to specify this as well?\n",
    "    source_dataset=source_data,\n",
    "    activation_store=store,\n",
    "    num_activations_before_training=max_items,\n",
    "    sweep_parameters=sweep_config,\n",
    "    num_iterations=num_iterations,\n",
    "    log_artifacts=True,\n",
    "    autoencoder=autoencoder,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "if wandb.run is None:\n",
    "    print(\"Weights & Biases (wandb) is not initialized.\")\n",
    "else:\n",
    "    print(\"Weights & Biases (wandb) is initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.encoder.Linear.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_weights = autoencoder.encoder.Linear.weight.T.detach().cpu()\n",
    "print(encoder_weights.shape)\n",
    "centred_weights = encoder_weights - encoder_weights.mean(dim=0)\n",
    "px.bar(encoder_weights.norm(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a cosine similarity matrix\n",
    "import torch.nn.functional as F\n",
    "from scipy.cluster import hierarchy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_cosine_sim_heatmap(centred_weights):\n",
    "    data_array = F.cosine_similarity(\n",
    "        centred_weights.T.unsqueeze(1), centred_weights.T.unsqueeze(0), dim=2\n",
    "    )\n",
    "    df = pd.DataFrame(data_array.numpy())\n",
    "\n",
    "    linkage = hierarchy.linkage(data_array)\n",
    "    dendrogram = hierarchy.dendrogram(linkage, no_plot=True, color_threshold=-np.inf)\n",
    "    reordered_ind = dendrogram[\"leaves\"]\n",
    "    # reorder df by ind\n",
    "    df = df.iloc[reordered_ind, reordered_ind]\n",
    "    data_array = df.to_numpy()\n",
    "    fig = px.imshow(\n",
    "        data_array, color_continuous_scale=\"RdBu\", color_continuous_midpoint=0\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "fig = get_cosine_sim_heatmap(centred_weights)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_weights = autoencoder.decoder[0].weight.detach().cpu()\n",
    "decoder_weights.shape\n",
    "\n",
    "centred_weights = decoder_weights - decoder_weights.mean(dim=0)\n",
    "px.bar(decoder_weights.norm(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = get_cosine_sim_heatmap(centred_weights)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at token intersection with encoder weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_alignment = src_model.W_E.cpu() @ encoder_weights.cpu()\n",
    "token_alignment = token_alignment.T.detach()\n",
    "token_alignment.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(token_alignment.norm(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"token\": token_strings,\n",
    "        \"projection\": token_alignment[442],\n",
    "    }\n",
    ")\n",
    "df.sort_values(\"projection\", ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_strings = tokenizer.convert_ids_to_tokens(list(tokenizer.vocab.values()))\n",
    "# sort the strings by the keys in tokenizer vocab\n",
    "token_strings = [x for _, x in sorted(zip(tokenizer.vocab.keys(), token_strings))]\n",
    "token_strings[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
